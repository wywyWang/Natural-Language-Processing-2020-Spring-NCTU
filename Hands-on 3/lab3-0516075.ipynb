{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab3-0516075.ipynb","provenance":[{"file_id":"https://gist.github.com/bshmueli/b85471ad4f92b45ab91bab2b0f895c5a#file-lab3-demo-short-ipynb","timestamp":1589089798019}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7HlI_7FQXftH","colab_type":"code","colab":{}},"source":["import re, math\n","from collections import Counter, defaultdict\n","import nltk\n","from nltk.tokenize import TweetTokenizer\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXP4NjhOJUMA","colab_type":"text"},"source":["# Part1:  Build a  2-gram model for the Twitter training data"]},{"cell_type":"markdown","metadata":{"id":"hythVqy6YqPw","colab_type":"text"},"source":["## Fetching the training and testing Corpus\n","`get_corpus()` reads the JSON file, and then return training and testing data in dataframe format"]},{"cell_type":"code","metadata":{"id":"tytiDrIbJUTp","colab_type":"code","colab":{}},"source":["def get_corpus():\n","    df_train = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_train.txt', lines=True)\n","    df_test = pd.read_json('https://raw.githubusercontent.com/bshmueli/108-nlp/master/tweets_test.txt', lines=True)\n","    print(\"Dataset training columns\", df_train.columns)\n","    print(\"Dataset training size\", len(df_train))\n","    print(\"Dataset testing columns\", df_test.columns)\n","    print(\"Dataset testing size\", len(df_test))\n","    return df_train, df_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aczdzJrWYz7J","colab_type":"text"},"source":["## Perfomring tokenizer"]},{"cell_type":"code","metadata":{"id":"LWnWu-k8Y0DI","colab_type":"code","colab":{}},"source":["def tokenize(document):\n","    tweet_tokenizer = TweetTokenizer()\n","    return tweet_tokenizer.tokenize(document.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5n261KwUY2I2","colab_type":"text"},"source":["## Count frequencies of vocabulary"]},{"cell_type":"code","metadata":{"id":"sELIDrTJY2PS","colab_type":"code","colab":{}},"source":["def count_vocab(corpus_tokenize):\n","    vocabulary = Counter()\n","    for document_tokenize in corpus_tokenize:\n","        vocabulary.update(document_tokenize)\n","    return vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hCMeb-RUY6a5","colab_type":"text"},"source":["## Get tokenize UNK corpus"]},{"cell_type":"code","metadata":{"id":"H6JzOxQlY6fK","colab_type":"code","colab":{}},"source":["def get_test_unk(test_tokenize, valid_vocab):\n","    test_tokenize_copy = test_tokenize.copy()\n","    corpus_tokenize_unk = []\n","    for document_tokenize in test_tokenize_copy:\n","        for token_id, token in enumerate(document_tokenize):\n","            if token not in valid_vocab:\n","                document_tokenize[token_id] = '<UNK>'\n","        corpus_tokenize_unk.append(document_tokenize)\n","    return corpus_tokenize_unk"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eSesGlleY9a6","colab_type":"text"},"source":["## Computing bigram frequencies\n","`get_ngram(corpus)` computes the bigram frequencies."]},{"cell_type":"code","metadata":{"id":"mfcSYK3TY9i2","colab_type":"code","colab":{}},"source":["def get_ngram(corpus_tokenize):\n","    vocabulary = defaultdict(lambda: defaultdict(lambda: 0))\n","    for document_tokenize in corpus_tokenize:\n","        twograms = nltk.ngrams(document_tokenize, 2)\n","        for w1, w2 in twograms:\n","            vocabulary[w1][w2] += 1\n","    return vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xb04ngsZALf","colab_type":"text"},"source":["## Compute perplexity"]},{"cell_type":"code","metadata":{"id":"2P4l3BNTZAR7","colab_type":"code","colab":{}},"source":["def compute_perplexity(corpus_tokenize_unk, bigram_model, V):\n","    perplexity = 0\n","    for document_id, document_tokenize in enumerate(corpus_tokenize_unk):\n","        N = len(list(nltk.ngrams(document_tokenize, 2)))\n","        probabilities = []\n","        for w1, w2 in nltk.ngrams(document_tokenize, 2):\n","            numerator = 1 + bigram_model[w1][w2]\n","            denominator = V + sum(bigram_model[w1].values())\n","            probabilities.append(numerator / denominator)\n","        cross_entropy = -1 / N * sum([math.log(p, 2) for p in probabilities])\n","        perplexity += math.pow(2, cross_entropy)\n","        if document_id % 20000 == 0:\n","            print(\"NOW IS: {}\".format(document_id))\n","            print(\"cross entropy: {}\".format(cross_entropy))\n","            print(\"perplexity: {}\".format(math.pow(2, cross_entropy)))\n","\n","    perplexity /= len(corpus_tokenize_unk)\n","    return perplexity"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuSyhYWGZCwE","colab_type":"text"},"source":["## Main program"]},{"cell_type":"code","metadata":{"id":"V9bI3NRNZC0O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"2aa2d586-9dc4-406a-ae1f-d21a43802b89","executionInfo":{"status":"ok","timestamp":1589295613759,"user_tz":-480,"elapsed":15272,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["df_train, df_test = get_corpus()\n","df_train_tokenize = [['<s>'] + tokenize(document) + ['</s>'] for document in df_train['text']]\n","df_test_tokenize = [['<s>'] + tokenize(document) + ['</s>'] for document in df_test['text']]"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Dataset training columns Index(['text'], dtype='object')\n","Dataset training size 132599\n","Dataset testing columns Index(['text'], dtype='object')\n","Dataset testing size 33137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TX0iX4thZF8Q","colab_type":"code","colab":{}},"source":["THRESHOLD = 3\n","train_vocab = count_vocab(df_train_tokenize)\n","# train_rare_vocab = [key for key, value in train_vocab.items() if value < THRESHOLD]\n","train_valid_vocab = [key for key, value in train_vocab.items() if value >= THRESHOLD]\n","df_train_tokenize_unk = get_test_unk(df_train_tokenize, train_valid_vocab)\n","test_vocab = count_vocab(df_test_tokenize)\n","df_test_tokenize_unk = get_test_unk(df_test_tokenize, train_valid_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"56NBkg6vZHOg","colab_type":"code","colab":{}},"source":["bigram_model = get_ngram(df_train_tokenize_unk)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWaxTMgdZJIL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":415},"outputId":"6a7ab67d-aa35-44b7-f094-9525c041a647","executionInfo":{"status":"ok","timestamp":1589295842297,"user_tz":-480,"elapsed":243799,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["V_train = len(train_valid_vocab)\n","training_perplexity = compute_perplexity(df_train_tokenize_unk, bigram_model, V_train)\n","print()\n","print(\"Training perplexity is: {}\".format(training_perplexity))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["NOW IS: 0\n","cross entropy: 11.788556698852915\n","perplexity: 3537.603638698599\n","NOW IS: 20000\n","cross entropy: 10.972909799373516\n","perplexity: 2009.9024950967184\n","NOW IS: 40000\n","cross entropy: 10.255117296199366\n","perplexity: 1222.075155849423\n","NOW IS: 60000\n","cross entropy: 8.943855448675427\n","perplexity: 492.45751236696424\n","NOW IS: 80000\n","cross entropy: 9.625676603888536\n","perplexity: 789.9823464670678\n","NOW IS: 100000\n","cross entropy: 8.61114157941023\n","perplexity: 391.0316516305096\n","NOW IS: 120000\n","cross entropy: 9.801505600818954\n","perplexity: 892.3745757896074\n","\n","Training perplexity is: 1355.335648828969\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-TtHKCG5ZMrl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":155},"outputId":"e8c1c3e8-3711-45fa-ca39-c04c206124ca","executionInfo":{"status":"ok","timestamp":1589295857125,"user_tz":-480,"elapsed":258624,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["testing_perplexity = compute_perplexity(df_test_tokenize_unk, bigram_model, V_train)\n","print()\n","print(\"Testing perplexity is: {}\".format(testing_perplexity))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["NOW IS: 0\n","cross entropy: 11.287989994561682\n","perplexity: 2500.4811164461944\n","NOW IS: 20000\n","cross entropy: 10.731215578398482\n","perplexity: 1699.8781009466716\n","\n","Testing perplexity is: 1640.5500188061494\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JI5ozfAdZPf6","colab_type":"text"},"source":["# Part2: Build a *bi-directional* 2-gram model by training on the Twitter training data"]},{"cell_type":"markdown","metadata":{"id":"yumMlNuxZQJg","colab_type":"text"},"source":["## Computing bigram frequencies\n","`get_ngram_part2(corpus)` computes the bigram frequencies."]},{"cell_type":"code","metadata":{"id":"uCoWb7SPZPkZ","colab_type":"code","colab":{}},"source":["def get_ngram_part2(corpus_tokenize):\n","    vocabulary_inverse = defaultdict(lambda: defaultdict(lambda: 0))\n","    for document_tokenize in corpus_tokenize:\n","        twograms_inverse = nltk.ngrams(document_tokenize, 2)\n","        for w1, w2 in twograms_inverse:\n","            vocabulary_inverse[w1][w2] += 1\n","    return vocabulary_inverse"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uAioYa-sZWSo","colab_type":"text"},"source":["## Compute perplexity of linear combination of bi-directional"]},{"cell_type":"code","metadata":{"id":"7kynzYvaZWdt","colab_type":"code","colab":{}},"source":["def compute_perplexity_part2(corpus_tokenize_unk, bigram_model, bigram_model_inverse, V, step):\n","    perplexity = 0\n","    for document_id, document_tokenize in enumerate(corpus_tokenize_unk):\n","        N = len(list(nltk.ngrams(document_tokenize, 2)))\n","        probabilities = []\n","        document_tokenize_copy = document_tokenize.copy()\n","        for token_id, token in enumerate(document_tokenize):\n","            if token == '<s>' or token == '</s>':\n","                continue\n","            else:\n","                numerator_forward = 1 + bigram_model[document_tokenize_copy[token_id-1]][token]\n","                denominator_forward = V + sum(bigram_model[document_tokenize_copy[token_id-1]].values())\n","                numerator_backward = 1 + bigram_model_inverse[document_tokenize_copy[token_id+1]][token]\n","                denominator_backward = V + sum(bigram_model_inverse[document_tokenize_copy[token_id+1]].values())\n","                probability = step * (numerator_forward / denominator_forward) + (1 - step) * (numerator_backward / denominator_backward)\n","                probabilities.append(probability)                \n","            \n","        cross_entropy = -1 / N * sum([math.log(p, 2) for p in probabilities])\n","        perplexity += math.pow(2, cross_entropy)\n","    perplexity /= len(corpus_tokenize_unk)\n","    return perplexity"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OyMs2SanZYqn","colab_type":"text"},"source":["## Main program"]},{"cell_type":"code","metadata":{"id":"VY4UQ72cZYv-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"3c52f76e-d7ec-4088-f107-8c2a09c383bf","executionInfo":{"status":"ok","timestamp":1589295857127,"user_tz":-480,"elapsed":258616,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["GAMMA = np.arange(0.0, 1.0, 0.05)\n","GAMMA"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n","       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"AiV4W3jhZdbA","colab_type":"code","colab":{}},"source":["df_train_tokenize_unk_inverse = [document[::-1] for document in df_train_tokenize_unk]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8875HB5QZegG","colab_type":"code","colab":{}},"source":["bigram_model_inverse = get_ngram(df_train_tokenize_unk_inverse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2RC9rVXZf9y","colab_type":"code","colab":{}},"source":["best_gamma = 0\n","best_training_perplexity = 0\n","best_testing_perplexity = 1e7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WI8PB9hVZgEP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3f9b3619-0534-42d7-a929-f17d8a9c6240","executionInfo":{"status":"ok","timestamp":1589299025893,"user_tz":-480,"elapsed":3427366,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["for gamma in GAMMA:\n","    print(\"NOW is: {}\".format(gamma))\n","    training_perplexity = compute_perplexity_part2(df_train_tokenize_unk, bigram_model, bigram_model_inverse, V_train, gamma)\n","    testing_perplexity = compute_perplexity_part2(df_test_tokenize_unk, bigram_model, bigram_model_inverse, V_train, gamma)\n","    if testing_perplexity <= best_testing_perplexity:\n","        best_gamma = gamma\n","        best_training_perplexity = training_perplexity\n","        best_testing_perplexity = testing_perplexity\n","        print(\"best gamma: {}\".format(best_gamma))\n","    print(\"training perplexity: {}\".format(training_perplexity))\n","    print(\"testing perplexity: {}\".format(testing_perplexity))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["NOW is: 0.0\n","best gamma: 0.0\n","training perplexity: 1001.4953832652383\n","testing perplexity: 1219.236398266508\n","NOW is: 0.05\n","best gamma: 0.05\n","training perplexity: 832.5586991748836\n","testing perplexity: 973.4204425041922\n","NOW is: 0.1\n","best gamma: 0.1\n","training perplexity: 771.9906794150176\n","testing perplexity: 893.4705837491047\n","NOW is: 0.15000000000000002\n","best gamma: 0.15000000000000002\n","training perplexity: 734.5527346243008\n","testing perplexity: 845.1529361185156\n","NOW is: 0.2\n","best gamma: 0.2\n","training perplexity: 708.6434475185696\n","testing perplexity: 812.1590047277458\n","NOW is: 0.25\n","best gamma: 0.25\n","training perplexity: 689.8998470099566\n","testing perplexity: 788.5124504418635\n","NOW is: 0.30000000000000004\n","best gamma: 0.30000000000000004\n","training perplexity: 676.1940179736641\n","testing perplexity: 771.3455403204982\n","NOW is: 0.35000000000000003\n","best gamma: 0.35000000000000003\n","training perplexity: 666.3477657766954\n","testing perplexity: 759.0886191161483\n","NOW is: 0.4\n","best gamma: 0.4\n","training perplexity: 659.6688996033531\n","testing perplexity: 750.8272000306605\n","NOW is: 0.45\n","best gamma: 0.45\n","training perplexity: 655.7512863066245\n","testing perplexity: 746.0279140227412\n","NOW is: 0.5\n","best gamma: 0.5\n","training perplexity: 654.3808701553082\n","testing perplexity: 744.4107434171606\n","NOW is: 0.55\n","training perplexity: 655.4929242283984\n","testing perplexity: 745.891165767439\n","NOW is: 0.6000000000000001\n","training perplexity: 659.1602752176005\n","testing perplexity: 750.564122881672\n","NOW is: 0.65\n","training perplexity: 665.6075002274679\n","testing perplexity: 758.7228352196803\n","NOW is: 0.7000000000000001\n","training perplexity: 675.2572551006721\n","testing perplexity: 770.9207660031584\n","NOW is: 0.75\n","training perplexity: 688.8311366212666\n","testing perplexity: 788.1072829509732\n","NOW is: 0.8\n","training perplexity: 707.5653847968864\n","testing perplexity: 811.9198334597241\n","NOW is: 0.8500000000000001\n","training perplexity: 733.7173086990609\n","testing perplexity: 845.3762933490829\n","NOW is: 0.9\n","training perplexity: 771.9966707302347\n","testing perplexity: 894.8564752811341\n","NOW is: 0.9500000000000001\n","training perplexity: 835.3276307174206\n","testing perplexity: 978.2321523158206\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q7Tx12biZhY6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"44e3d98e-5dc0-4aa8-f145-1c7b3936811e","executionInfo":{"status":"ok","timestamp":1589299025895,"user_tz":-480,"elapsed":3427364,"user":{"displayName":"威堯王","photoUrl":"","userId":"08800868367404322837"}}},"source":["print(\"best gamma: {}\".format(best_gamma))\n","print(\"best training perplexity: {}\".format(best_training_perplexity))\n","print(\"best testing perplexity: {}\".format(best_testing_perplexity))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["best gamma: 0.5\n","best training perplexity: 654.3808701553082\n","best testing perplexity: 744.4107434171606\n"],"name":"stdout"}]}]}